import json
from typing import List

import regex
import unicodedata

_REGEX_GRAPHEME = regex.compile(r'\X', flags=regex.UNICODE)
_REGEX_WORD_CHAR = regex.compile(r'\w', flags=regex.UNICODE)

#
_ASCII_ALIKE = {'A': 'Ã€ÃÃ‚ÃƒÃ„Ã…Ä€Ä‚Ä„ÇÇÇ ÇºÈ€È‚È¦ÈºÎ†Î‘á´€á´¬á¸€áº áº¢áº¤áº¦áº¨áºªáº¬áº®áº°áº²áº´áº¶á¼ˆá¼‰á¼Šá¼‹á¼Œá¼á¼á¼á¾ˆá¾‰á¾Šá¾‹á¾Œá¾á¾á¾á¾¸á¾¹á¾ºá¾»á¾¼â±¯ï¼¡ğ€ğ´ğ‘¨ğ’œğ“ğ”„ğ”¸ğ•¬ğ– ğ—”ğ˜ˆğ˜¼ğ™°',
                'B': 'ÆÆ‚ÉƒÊ™Î’á´ƒá´®á´¯á¸‚á¸„á¸†â„¬ï¼¢ğğµğ‘©ğ“‘ğ”…ğ”¹ğ•­ğ–¡ğ—•ğ˜‰ğ˜½ğ™±',
                'C': 'Ã‡Ä†ÄˆÄŠÄŒÆ‡È»Ê—á´„á¸ˆâ„‚â„­ï¼£ğ‚ğ¶ğ‘ªğ’ğ“’ğ•®ğ–¢ğ—–ğ˜Šğ˜¾ğ™²',
                'D': 'ÃÄÄÆ‰ÆŠÆ‹Î”á´…á´†á´°á¸Šá¸Œá¸á¸á¸’â……ï¼¤ğƒğ·ğ‘«ğ’Ÿğ““ğ”‡ğ”»ğ•¯ğ–£ğ——ğ˜‹ğ˜¿ğ™³',
                'E': 'ÃˆÃ‰ÃŠÃ‹Ä’Ä”Ä–Ä˜ÄšÆÈ„È†È¨É†ÊšÎˆÎ‰Î•Î—á´‡á´±á´²á¸”á¸–á¸˜á¸šá¸œáº¸áººáº¼áº¾á»€á»‚á»„á»†á¼˜á¼™á¼šá¼›á¼œá¼á¼¨á¼©á¼ªá¼«á¼¬á¼­á¼®á¼¯á¾˜á¾™á¾šá¾›á¾œá¾á¾á¾Ÿá¿ˆá¿‰á¿Šá¿‹á¿Œâ„°ï¼¥ğ„ğ¸ğ‘¬ğ“”ğ”ˆğ”¼ğ•°ğ–¤ğ—˜ğ˜Œğ™€ğ™´',
                'F': 'Æ‘É¸á¸â„±ï¼¦ğ…ğ¹ğ‘­ğ“•ğ”‰ğ”½ğ•±ğ–¥ğ—™ğ˜ğ™ğ™µ',
                'G': 'ÄœÄÄ Ä¢Æ“Æ”Ç¤Ç¦Ç´Ê›Ë Î“á´³á¸ ï¼§ğ†ğºğ‘®ğ’¢ğ“–ğ”Šğ”¾ğ•²ğ–¦ğ—šğ˜ğ™‚ğ™¶',
                'H': 'Ä¤Ä¦ÈÊœá´´á¸¢á¸¤á¸¦á¸¨á¸ªâ„‹â„Œâ„â±§ï¼¨ğ‡ğ»ğ‘¯ğ“—ğ•³ğ–§ğ—›ğ˜ğ™ƒğ™·',
                'I': 'ÃŒÃÃÃÄ¨ÄªÄ¬Ä®Ä°Æ–Æ—ÇÈˆÈŠÉªÎŠÎÎ™Îªá´µá¸¬á¸®á»ˆá»Šá¼¸á¼¹á¼ºá¼»á¼¼á¼½á¼¾á¼¿á¿˜á¿™á¿šá¿›â„â„‘ï¼©ğˆğ¼ğ‘°ğ“˜ğ•€ğ•´ğ–¨ğ—œğ˜ğ™„ğ™¸',
                'J': 'Ä´Éˆá´Šá´¶ï¼ªğ‰ğ½ğ‘±ğ’¥ğ“™ğ”ğ•ğ•µğ–©ğ—ğ˜‘ğ™…ğ™¹',
                'K': 'Ä¶Æ˜Ç¨Îšá´‹á´·á¸°á¸²á¸´â±©ï¼«ğŠğ¾ğ‘²ğ’¦ğ“šğ”ğ•‚ğ•¶ğ–ªğ—ğ˜’ğ™†ğ™º',
                'L': 'Ä¹Ä»Ä½Ä¿ÅÈ½ÊŸÎ›á´Œá´¸á¸¶á¸¸á¸ºá¸¼â„’â± â±¢ï¼¬ğ‹ğ¿ğ‘³ğ“›ğ”ğ•ƒğ•·ğ–«ğ—Ÿğ˜“ğ™‡ğ™»',
                'M': 'Îœá´á´¹á¸¾á¹€á¹‚â„³â±®ï¼­ğŒğ‘€ğ‘´ğ“œğ”ğ•„ğ•¸ğ–¬ğ— ğ˜”ğ™ˆğ™¼',
                'N': 'Ã‘ÅƒÅ…Å‡ÆÇ¸È Îá´á´ºá´»á¹„á¹†á¹ˆá¹Šâ„•ï¼®ğğ‘ğ‘µğ’©ğ“ğ”‘ğ•¹ğ–­ğ—¡ğ˜•ğ™‰ğ™½',
                'O': 'Ã’Ã“Ã”Ã•Ã–Ã˜ÅŒÅÅÆ†ÆŸÆ Ç‘ÇªÇ¬Ç¾ÈŒÈÈªÈ¬È®È°É·ÎŒÎÎŸÎ©á´á´‘á´“á´¼á¹Œá¹á¹á¹’á»Œá»á»á»’á»”á»–á»˜á»šá»œá»á» á»¢á½ˆá½‰á½Šá½‹á½Œá½á½¨á½©á½ªá½«á½¬á½­á½®á½¯á¾¨á¾©á¾ªá¾«á¾¬á¾­á¾®á¾¯á¿¸á¿¹á¿ºá¿»á¿¼ï¼¯ğğ‘‚ğ‘¶ğ’ªğ“ğ”’ğ•†ğ•ºğ–®ğ—¢ğ˜–ğ™Šğ™¾',
                'P': 'Æ¤Î á´˜á´¾á¹”á¹–â„™â±£ï¼°ğğ‘ƒğ‘·ğ’«ğ“Ÿğ”“ğ•»ğ–¯ğ—£ğ˜—ğ™‹ğ™¿',
                'Q': 'Ïâ„šï¼±ğğ‘„ğ‘¸ğ’¬ğ“ ğ””ğ•¼ğ–°ğ—¤ğ˜˜ğ™Œğš€',
                'R': 'Å”Å–Å˜ÈÈ’ÉŒÊ€ÊÎ¡á´™á´šá´¿á¹˜á¹šá¹œá¹á¿¤á¿¥á¿¬â„›â„œâ„â±¤ï¼²ğ‘ğ‘…ğ‘¹ğ“¡ğ•½ğ–±ğ—¥ğ˜™ğ™ğš',
                'S': 'ÅšÅœÅÅ È˜ÊƒÊ…Ê†Î£á¹ á¹¢á¹¤á¹¦á¹¨áº›ï¼³ğ’ğ‘†ğ‘ºğ’®ğ“¢ğ”–ğ•Šğ•¾ğ–²ğ—¦ğ˜šğ™ğš‚',
                'T': 'Å¢Å¤Å¦Æ¬Æ®ÈšÈ¾Î¤á´›áµ€á¹ªá¹¬á¹®á¹°ï¼´ğ“ğ‘‡ğ‘»ğ’¯ğ“£ğ”—ğ•‹ğ•¿ğ–³ğ—§ğ˜›ğ™ğšƒ',
                'U': 'Ã™ÃšÃ›ÃœÅ¨ÅªÅ¬Å®Å°Å²Æ¯Ç“Ç•Ç—Ç™Ç›È”È–É„ÊŠÎÎ¥Î«Ï’Ï“Ï”á´œáµá¹²á¹´á¹¶á¹¸á¹ºá»¤á»¦á»¨á»ªá»¬á»®á»°á½™á½›á½á½Ÿá¿¨á¿©á¿ªá¿«ï¼µğ”ğ‘ˆğ‘¼ğ’°ğ“¤ğ”˜ğ•Œğ–€ğ–´ğ—¨ğ˜œğ™ğš„',
                'V': 'Æ²Ë¬á´ á¹¼á¹¾ï¼¶ğ•ğ‘‰ğ‘½ğ’±ğ“¥ğ”™ğ•ğ–ğ–µğ—©ğ˜ğ™‘ğš…',
                'W': 'Å´ÆœÇ·É¯É°Ïœá´¡áµ‚áº€áº‚áº„áº†áºˆï¼·ğ–ğ‘Šğ‘¾ğ’²ğ“¦ğ”šğ•ğ–‚ğ–¶ğ—ªğ˜ğ™’ğš†',
                'X': 'áºŠáºŒï¼¸ğ—ğ‘‹ğ‘¿ğ’³ğ“§ğ”›ğ•ğ–ƒğ–·ğ—«ğ˜Ÿğ™“ğš‡',
                'Y': 'ÃÅ¶Å¸Æ±Æ³ÈœÈ²ÉÉ¥Êáºá»²á»´á»¶á»¸ï¼¹ğ˜ğ‘Œğ’€ğ’´ğ“¨ğ”œğ•ğ–„ğ–¸ğ—¬ğ˜ ğ™”ğšˆ',
                'Z': 'Å¹Å»Å½ÆµÈ¤Ê’Ê“Î–á´¢áºáº’áº”â„¤â„¨â±«ï¼ºğ™ğ‘ğ’ğ’µğ“©ğ–…ğ–¹ğ—­ğ˜¡ğ™•ğš‰',
                'a': 'Ã Ã¡Ã¢Ã£Ã¤Ã¥ÄÄƒÄ…ÇÇŸÇ¡Ç»ÈÈƒÈ§ÉÉ‘É’Î¬Î±áµƒáµ„áµ…á¸áºšáº¡áº£áº¥áº§áº©áº«áº­áº¯áº±áº³áºµáº·á¼€á¼á¼‚á¼ƒá¼„á¼…á¼†á¼‡á½°á½±á¾€á¾á¾‚á¾ƒá¾„á¾…á¾†á¾‡á¾°á¾±á¾²á¾³á¾´á¾¶á¾·â‚â±¥ï½ğšğ‘ğ’‚ğ’¶ğ“ªğ”ğ•’ğ–†ğ–ºğ—®ğ˜¢ğ™–ğšŠ',
                'b': 'Æ€ÆƒÉ“Î²Ïáµ‡áµáµ¦áµ¬á¶€á¸ƒá¸…á¸‡ï½‚ğ›ğ‘ğ’ƒğ’·ğ“«ğ”Ÿğ•“ğ–‡ğ–»ğ—¯ğ˜£ğ™—ğš‹',
                'c': 'Ã§Ä‡Ä‰Ä‹ÄÆˆÈ¼É•Ï²á¸‰ï½ƒğœğ‘ğ’„ğ’¸ğ“¬ğ” ğ•”ğ–ˆğ–¼ğ—°ğ˜¤ğ™˜ğšŒ',
                'd': 'Ã°ÄÄ‘ÆŒÆÈ¡É–É—Î´áµˆáµŸáµ­á¶á¸‹á¸á¸á¸‘á¸“â…†ï½„ğğ‘‘ğ’…ğ’¹ğ“­ğ”¡ğ••ğ–‰ğ–½ğ—±ğ˜¥ğ™™ğš',
                'e': 'Ã¨Ã©ÃªÃ«Ä“Ä•Ä—Ä™Ä›È…È‡È©É‡É˜É›ÉœÉÉÎ­Î®ÎµÎ·á´ˆáµ‰áµ‹áµŒá¸•á¸—á¸™á¸›á¸áº¹áº»áº½áº¿á»á»ƒá»…á»‡á¼á¼‘á¼’á¼“á¼”á¼•á¼ á¼¡á¼¢á¼£á¼¤á¼¥á¼¦á¼§á½²á½³á½´á½µá¾á¾‘á¾’á¾“á¾”á¾•á¾–á¾—á¿‚á¿ƒá¿„á¿†á¿‡â‚‘â„¯â…‡ï½…ğğ‘’ğ’†ğ“®ğ”¢ğ•–ğ–Šğ–¾ğ—²ğ˜¦ğ™šğš',
                'f': 'Æ’áµ áµ©áµ®á¶‚á¸Ÿï½†ğŸğ‘“ğ’‡ğ’»ğ“¯ğ”£ğ•—ğ–‹ğ–¿ğ—³ğ˜§ğ™›ğš',
                'g': 'ÄÄŸÄ¡Ä£Ç¥Ç§ÇµÉ É¡É¢É£Î³áµáµáµ§áµ·á¶ƒá¸¡â„Šï½‡ğ ğ‘”ğ’ˆğ“°ğ”¤ğ•˜ğ–Œğ—€ğ—´ğ˜¨ğ™œğš',
                'h': 'Ä¥Ä§ÈŸÉ¦É§Ê®Ê¯Ê°Ê±á¸£á¸¥á¸§á¸©á¸«áº–â‚•â±¨ï½ˆğ¡ğ’‰ğ’½ğ“±ğ”¥ğ•™ğ–ğ—ğ—µğ˜©ğ™ğš‘',
                'i': 'Ã¬Ã­Ã®Ã¯Ä©Ä«Ä­Ä¯Ä±ÇÈ‰È‹É¨É©Î¯Î¹ÏŠá´‰áµáµ¢á¸­á¸¯á»‰á»‹á¼°á¼±á¼²á¼³á¼´á¼µá¼¶á¼·á½¶á½·á¾¾á¿á¿‘á¿’á¿“á¿–á¿—â±â…ˆï½‰ğ¢ğ‘–ğ’Šğ’¾ğ“²ğ”¦ğ•šğ–ğ—‚ğ—¶ğ˜ªğ™ğš’ğš¤',
                'j': 'ÄµÇ°È·É‰ÉŸÊ„ÊÊ²Ï³â…‰ï½Šğ£ğ‘—ğ’‹ğ’¿ğ“³ğ”§ğ•›ğ–ğ—ƒğ—·ğ˜«ğ™Ÿğš“ğš¥',
                'k': 'Ä·Ä¸Æ™Ç©ÊÎºÏ°áµá¶„á¸±á¸³á¸µâ‚–â±ªï½‹ğ¤ğ‘˜ğ’Œğ“€ğ“´ğ”¨ğ•œğ–ğ—„ğ—¸ğ˜¬ğ™ ğš”',
                'l': 'ÄºÄ¼Ä¾Å€Å‚ÆšÆ›È´É«É¬É­Ë¡Î»á¶…á¸·á¸¹á¸»á¸½â‚—â„“â±¡ï½Œğ¥ğ‘™ğ’ğ“ğ“µğ”©ğ•ğ–‘ğ—…ğ—¹ğ˜­ğ™¡ğš•',
                'm': 'É±Î¼á´Ÿáµáµšáµ¯á¶†á¸¿á¹á¹ƒâ‚˜ï½ğ¦ğ‘šğ’ğ“‚ğ“¶ğ”ªğ•ğ–’ğ—†ğ—ºğ˜®ğ™¢ğš–',
                'n': 'Ã±Å„Å†ÅˆÆÇ¹ÈµÉ²É³É´Î½áµ°á¶‡á¹…á¹‡á¹‰á¹‹â¿â‚™ï½ğ§ğ‘›ğ’ğ“ƒğ“·ğ”«ğ•Ÿğ–“ğ—‡ğ—»ğ˜¯ğ™£ğš—',
                'o': 'Ã²Ã³Ã´ÃµÃ¶Ã¸ÅÅÅ‘Æ¡Ç’Ç«Ç­Ç¿ÈÈÈ«È­È¯È±É”ÉµÎ¿Ï‰ÏŒÏáµ’á¹á¹á¹‘á¹“á»á»á»‘á»“á»•á»—á»™á»›á»á»Ÿá»¡á»£á½€á½á½‚á½ƒá½„á½…á½ á½¡á½¢á½£á½¤á½¥á½¦á½§á½¸á½¹á½¼á½½á¾ á¾¡á¾¢á¾£á¾¤á¾¥á¾¦á¾§á¿²á¿³á¿´á¿¶á¿·â‚’â„´ï½ğ¨ğ‘œğ’ğ“¸ğ”¬ğ• ğ–”ğ—ˆğ—¼ğ˜°ğ™¤ğš˜',
                'p': 'Æ¥Ï€Ï–áµ–áµ±áµ½á¶ˆá¹•á¹—â‚šï½ğ©ğ‘ğ’‘ğ“…ğ“¹ğ”­ğ•¡ğ–•ğ—‰ğ—½ğ˜±ğ™¥ğš™',
                'q': 'ÉŠÉ‹Ê ÏŸï½‘ğªğ‘ğ’’ğ“†ğ“ºğ”®ğ•¢ğ––ğ—Šğ—¾ğ˜²ğ™¦ğšš',
                'r': 'Å•Å—Å™È‘È“ÉÉ¹ÉºÉ»É¼É½É¾É¿Ê³Ê´ÊµÊ¶ÏÏ±áµ£áµ¨áµ²áµ³á¶‰á¹™á¹›á¹á¹Ÿï½’ğ«ğ‘Ÿğ’“ğ“‡ğ“»ğ”¯ğ•£ğ–—ğ—‹ğ—¿ğ˜³ğ™§ğš›',
                's': 'Å›ÅÅŸÅ¡Å¿È™È¿Ê‚Ë¢Ï‚Ïƒáµ´á¶Šá¹¡á¹£á¹¥á¹§á¹©â‚›ï½“ğ¬ğ‘ ğ’”ğ“ˆğ“¼ğ”°ğ•¤ğ–˜ğ—Œğ˜€ğ˜´ğ™¨ğšœ',
                't': 'Å£Å¥Å§Æ«Æ­È›È¶Ê‡ÊˆÏ„áµ—áµµá¹«á¹­á¹¯á¹±áº—â‚œâ±¦ï½”ğ­ğ‘¡ğ’•ğ“‰ğ“½ğ”±ğ•¥ğ–™ğ—ğ˜ğ˜µğ™©ğš',
                'u': 'Ã¹ÃºÃ»Ã¼Å©Å«Å­Å¯Å±Å³Æ°Ç”Ç–Ç˜ÇšÇœÈ•È—É¤Ê‰Î°Ï…Ï‹Ïá´á´áµ˜áµ™áµ¤á¹³á¹µá¹·á¹¹á¹»á»¥á»§á»©á»«á»­á»¯á»±á½á½‘á½’á½“á½”á½•á½–á½—á½ºá½»á¿ á¿¡á¿¢á¿£á¿¦á¿§ï½•ğ®ğ‘¢ğ’–ğ“Šğ“¾ğ”²ğ•¦ğ–šğ—ğ˜‚ğ˜¶ğ™ªğš',
                'v': 'Ê‹áµ›áµ¥á¶Œá¹½á¹¿ï½–ğ¯ğ‘£ğ’—ğ“‹ğ“¿ğ”³ğ•§ğ–›ğ—ğ˜ƒğ˜·ğ™«ğšŸ',
                'w': 'ÅµÆ¿ÊÊ·Ïáºáºƒáº…áº‡áº‰áº˜ï½—ğ°ğ‘¤ğ’˜ğ“Œğ”€ğ”´ğ•¨ğ–œğ—ğ˜„ğ˜¸ğ™¬ğš ',
                'x': 'Ë£Î¾á¶áº‹áºâ‚“ï½˜ğ±ğ‘¥ğ’™ğ“ğ”ğ”µğ•©ğ–ğ—‘ğ˜…ğ˜¹ğ™­ğš¡',
                'y': 'Ã½Ã¿Å·Æ´ÈÈ³ÉÊÊ¸áºáº™á»³á»µá»·á»¹ï½™ğ²ğ‘¦ğ’šğ“ğ”‚ğ”¶ğ•ªğ–ğ—’ğ˜†ğ˜ºğ™®ğš¢',
                'z': 'ÅºÅ¼Å¾Æ¶È¥É€ÊÊ‘Î¶áµ¶á¶áº‘áº“áº•â±¬ï½šğ³ğ‘§ğ’›ğ“ğ”ƒğ”·ğ•«ğ–Ÿğ—“ğ˜‡ğ˜»ğ™¯ğš£',
                }
_ASCII_TRANLATION_LOOKUP = {ord(char): alpha for alpha, chars in _ASCII_ALIKE.items() for char in chars}


def _preprocess(text: str,
                nfkd: bool = False,
                casefold: bool = False,
                replace_ascii: bool = False,
                ) -> str:
    # sanity check
    if not isinstance(text, str):
        raise TypeError(f'expected <str>, got <{type(text)}>')

    # pre-process step 1: unicode decomposition
    if nfkd:
        text = f'{unicodedata.normalize("NFKD", text)}\0'
    else:
        text += '\0'

    # pre-process step 2: casefold
    if casefold:
        text = text.casefold()

    # pre-process step 3: replace ascii-like chars
    if replace_ascii:
        text = text.translate(_ASCII_TRANLATION_LOOKUP)

    return text


def tokenize(text: str,
             nfkd: bool = False,
             casefold: bool = False,
             replace_ascii: bool = False,
             strip_diacritics: bool = False,
             ) -> List[str]:
    """
    tokenize text into words
    * enable `nfkd` if you want to do string matching
    * enable `casefold` if you want case-insensitivity
    * enable `replace_ascii` if you want to match ascii strings against ascii-alike text
    * enable `strip_diacritics` if you want to fix zalgo text or want to ignore accents

    warning: if any flags are enabled, the output tokens may not be a substring of the input text

    :param text: to extract words from
    :param nfkd: unicode normal form compatibility decomposition
    :param casefold: lowercase but better
    :param replace_ascii: make ascii-like where possible
    :param strip_diacritics: unwrap graphemes and only keep base char
    :return: list of words
    """

    # pre-process (and sanity check)
    text = _preprocess(text, nfkd=nfkd, casefold=casefold, replace_ascii=replace_ascii)

    # get all word tokens
    tokens = []
    token_buffer = []
    for match in _REGEX_GRAPHEME.finditer(text):
        grapheme = match.group(0)

        # don't count underscore as a word token, despite what the unicode standard says
        if grapheme[0] not in {'_'}:
            if _REGEX_WORD_CHAR.match(grapheme):
                token_buffer.append(grapheme[0] if strip_diacritics else grapheme)
                continue

        # not a word-like grapheme, append current word
        if token_buffer:
            tokens.append(''.join(token_buffer))
            token_buffer.clear()

    return tokens


if __name__ == '__main__':
    print(json.dumps(tokenize('ğ‡ğğ¥ğ¥ğ¨ ğ–ğ¨ğ«ğ¥ğ')))
    print(json.dumps(tokenize('ğ—›ğ—²ğ—¹ğ—¹ğ—¼ ğ—ªğ—¼ğ—¿ğ—¹ğ—±')))
    print(json.dumps(tokenize('ğ»ğ‘’ğ‘™ğ‘™ğ‘œ ğ‘Šğ‘œğ‘Ÿğ‘™ğ‘‘')))
    print(json.dumps(tokenize('ğ˜ğ˜¦ğ˜­ğ˜­ğ˜° ğ˜ğ˜°ğ˜³ğ˜­ğ˜¥')))
    print(json.dumps(tokenize('ğ‘¯ğ’†ğ’ğ’ğ’ ğ‘¾ğ’ğ’“ğ’ğ’…')))
    print(json.dumps(tokenize('ğ™ƒğ™šğ™¡ğ™¡ğ™¤ ğ™’ğ™¤ğ™§ğ™¡ğ™™')))
