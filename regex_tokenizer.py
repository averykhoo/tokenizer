import json
from typing import Dict
from typing import List
from typing import Pattern
from typing import Union

import regex
import unicodedata

_REGEX_GRAPHEME: Pattern = regex.compile(r'\X', flags=regex.UNICODE)  # builtins.re does not support `\X`
_REGEX_WORD_CHAR: Pattern = regex.compile(r'\w', flags=regex.UNICODE)

_ASCII_ALIKE: Dict[str, str] = {
    # todo: missing bold digits e.g. U+1D7CF, see https://www.compart.com/en/unicode/block/U+1D400
    '0': 'â°â‚€â“ªâ“¿',
    '1': 'Â¹â‚â‘ â‘´â“µ',
    '2': 'Â²â‚‚â‘¡â‘µâ“¶',
    '3': 'Â³â‚ƒâ‘¢â‘¶â“·',
    '4': 'â´â‚„â‘£â‘·â“¸',
    '5': 'âµâ‚…â‘¤â‘¸â“¹',
    '6': 'â¶â‚†â‘¥â‘¹â“º',
    '7': 'â·â‚‡â‘¦â‘ºâ“»',
    '8': 'â¸â‚ˆâ‘§â‘»â“¼',
    '9': 'â¹â‚‰â‘¨â‘¼â“½',
    'A': 'Ã€ÃÃ‚ÃƒÃ„Ã…Ä€Ä‚Ä„ÇÇžÇ ÇºÈ€È‚È¦ÈºÎ†Î‘á´€á´¬á¸€áº áº¢áº¤áº¦áº¨áºªáº¬áº®áº°áº²áº´áº¶á¼ˆá¼‰á¼Šá¼‹á¼Œá¼á¼Žá¼á¾ˆá¾‰á¾Šá¾‹á¾Œá¾á¾Žá¾á¾¸á¾¹á¾ºá¾»á¾¼â„«âˆ€â’¶â±¯ï¼¡ð€ð´ð‘¨ð’œð“ð”„ð”¸ð•¬ð– ð—”ð˜ˆð˜¼ð™°ðŸ„ðŸ„°ðŸ…ðŸ…°ðŸ‡¦',
    'B': 'ÆÆ‚ÉƒÊ™Î’á´ƒá´®á´¯á¸‚á¸„á¸†â„¬â’·ï¼¢ððµð‘©ð“‘ð”…ð”¹ð•­ð–¡ð—•ð˜‰ð˜½ð™±ðŸ„‘ðŸ„±ðŸ…‘ðŸ…±ðŸ‡§',
    'C': 'Ã‡Ä†ÄˆÄŠÄŒÆ‡È»Ê—á´„á¸ˆâ„‚â„­âˆâ’¸ï¼£ð‚ð¶ð‘ªð’žð“’ð•®ð–¢ð—–ð˜Šð˜¾ð™²ðŸ„’ðŸ„«ðŸ„²ðŸ…’ðŸ…²ðŸ‡¨',
    'D': 'ÃÄŽÄÆ‰ÆŠÆ‹á´…á´†á´°á¸Šá¸Œá¸Žá¸á¸’â……â’¹ï¼¤ðƒð·ð‘«ð’Ÿð““ð”‡ð”»ð•¯ð–£ð——ð˜‹ð˜¿ð™³ðŸ„“ðŸ„³ðŸ…“ðŸ…³ðŸ‡©',
    'E': 'ÃˆÃ‰ÃŠÃ‹Ä’Ä”Ä–Ä˜ÄšÆÈ„È†È¨É†ÊšÎˆÎ•á´‡á´±á´²á¸”á¸–á¸˜á¸šá¸œáº¸áººáº¼áº¾á»€á»‚á»„á»†á¼˜á¼™á¼šá¼›á¼œá¼á¿ˆá¿‰â„°âˆƒâ’ºï¼¥ð„ð¸ð‘¬ð“”ð”ˆð”¼ð•°ð–¤ð—˜ð˜Œð™€ð™´ðŸ„”ðŸ„´ðŸ…”ðŸ…´ðŸ‡ª',
    'F': 'Æ‘á¸žâ„±â„²â…Žâ’»ï¼¦ð…ð¹ð‘­ð“•ð”‰ð”½ð•±ð–¥ð—™ð˜ð™ð™µðŸ„•ðŸ„µðŸ…•ðŸ…µðŸ‡«',
    'G': 'ÄœÄžÄ Ä¢Æ“Ç¤Ç¦Ç´Ê›á´³á¸ â’¼ï¼§ð†ðºð‘®ð’¢ð“–ð”Šð”¾ð•²ð–¦ð—šð˜Žð™‚ð™¶ðŸ„–ðŸ„¶ðŸ…–ðŸ…¶ðŸ‡¬',
    'H': 'Ä¤Ä¦ÈžÊœá´´á¸¢á¸¤á¸¦á¸¨á¸ªâ„‹â„Œâ„â’½â±§ï¼¨ð‡ð»ð‘¯ð“—ð•³ð–§ð—›ð˜ð™ƒð™·ðŸ„—ðŸ„·ðŸ…—ðŸ…·ðŸ‡­',
    'I': 'ÃŒÃÃŽÃÄ¨ÄªÄ¬Ä®Ä°Æ–Æ—ÇÈˆÈŠÉªÎŠÎÎ™Îªá´µá¸¬á¸®á»ˆá»Šá¼¸á¼¹á¼ºá¼»á¼¼á¼½á¼¾á¼¿á¿˜á¿™á¿šá¿›â„â„‘â’¾ï¼©ðˆð¼ð‘°ð“˜ð•€ð•´ð–¨ð—œð˜ð™„ð™¸ðŸ„˜ðŸ„¸ðŸ…˜ðŸ…¸ðŸ‡®',
    'J': 'Ä´Éˆá´Šá´¶â’¿ï¼ªð‰ð½ð‘±ð’¥ð“™ð”ð•ð•µð–©ð—ð˜‘ð™…ð™¹ðŸ„™ðŸ„¹ðŸ…™ðŸ…¹ðŸ‡¯',
    'K': 'Ä¶Æ˜Ç¨Îšá´‹á´·á¸°á¸²á¸´â„ªâ“€â±©ï¼«ðŠð¾ð‘²ð’¦ð“šð”Žð•‚ð•¶ð–ªð—žð˜’ð™†ð™ºðŸ„šðŸ„ºðŸ…šðŸ…ºðŸ‡°',
    'L': 'Ä¹Ä»Ä½Ä¿ÅÈ½ÊŸá´Œá´¸á¸¶á¸¸á¸ºá¸¼â„’â“â± â±¢ï¼¬ð‹ð¿ð‘³ð“›ð”ð•ƒð•·ð–«ð—Ÿð˜“ð™‡ð™»ðŸ„›ðŸ„»ðŸ…›ðŸ…»ðŸ‡±',
    'M': 'Îœá´á´¹á¸¾á¹€á¹‚â„³â“‚â±®ï¼­ðŒð‘€ð‘´ð“œð”ð•„ð•¸ð–¬ð— ð˜”ð™ˆð™¼ðŸ„œðŸ„¼ðŸ…œðŸ…¼ðŸ‡²',
    'N': 'Ã‘ÅƒÅ…Å‡ÆÇ¸È Îá´Žá´ºá´»á¹„á¹†á¹ˆá¹Šâ„•â“ƒï¼®ðð‘ð‘µð’©ð“ð”‘ð•¹ð–­ð—¡ð˜•ð™‰ð™½ðŸ„ðŸ„½ðŸ…ðŸ…½ðŸ‡³',
    'O': 'Ã’Ã“Ã”Ã•Ã–Ã˜ÅŒÅŽÅÆŸÆ Ç‘ÇªÇ¬Ç¾ÈŒÈŽÈªÈ¬È®È°ÎŒÎŸá´á´‘á´“á´¼á¹Œá¹Žá¹á¹’á»Œá»Žá»á»’á»”á»–á»˜á»šá»œá»žá» á»¢á½ˆá½‰á½Šá½‹á½Œá½á¿¸á¿¹â“„ï¼¯ðŽð‘‚ð‘¶ð’ªð“žð”’ð•†ð•ºð–®ð—¢ð˜–ð™Šð™¾ðŸ„žðŸ„¾ðŸ…žðŸ…¾ðŸ‡´',
    'P': 'Æ¤á´˜á´¾á¹”á¹–â„™â“…â±£ï¼°ðð‘ƒð‘·ð’«ð“Ÿð”“ð•»ð–¯ð—£ð˜—ð™‹ð™¿ðŸ„ŸðŸ„¿ðŸ…ŸðŸ…¿ðŸ‡µ',
    'Q': 'â„šâ“†ï¼±ðð‘„ð‘¸ð’¬ð“ ð””ð•¼ð–°ð—¤ð˜˜ð™Œðš€ðŸ„ ðŸ…€ðŸ… ðŸ†€ðŸ‡¶',
    'R': 'Å”Å–Å˜ÈÈ’ÉŒÊ€Êá´™á´šá´¿á¹˜á¹šá¹œá¹žá¿¬â„›â„œâ„â“‡â±¤ï¼²ð‘ð‘…ð‘¹ð“¡ð•½ð–±ð—¥ð˜™ð™ðšðŸ„¡ðŸ„¬ðŸ…ðŸ…¡ðŸ†ðŸ‡·',
    'S': 'ÅšÅœÅžÅ È˜ÊƒÊ…Ê†Î£á¹ á¹¢á¹¤á¹¦á¹¨áº›â“ˆï¼³ð’ð‘†ð‘ºð’®ð“¢ð”–ð•Šð•¾ð–²ð—¦ð˜šð™Žðš‚ðŸ„¢ðŸ„ªðŸ…‚ðŸ…¢ðŸ†‚ðŸ‡¸',
    'T': 'Å¢Å¤Å¦Æ¬Æ®ÈšÈ¾Î¤á´›áµ€á¹ªá¹¬á¹®á¹°âŠ¤âŠ¥â“‰ï¼´ð“ð‘‡ð‘»ð’¯ð“£ð”—ð•‹ð•¿ð–³ð—§ð˜›ð™ðšƒðŸ„£ðŸ…ƒðŸ…£ðŸ†ƒðŸ‡¹',
    'U': 'Ã™ÃšÃ›ÃœÅ¨ÅªÅ¬Å®Å°Å²Æ¯Ç“Ç•Ç—Ç™Ç›È”È–É„ÊŠá´œáµá¹²á¹´á¹¶á¹¸á¹ºá»¤á»¦á»¨á»ªá»¬á»®á»°â“Šï¼µð”ð‘ˆð‘¼ð’°ð“¤ð”˜ð•Œð–€ð–´ð—¨ð˜œð™ðš„ðŸ„¤ðŸ…„ðŸ…¤ðŸ†„ðŸ‡º',
    'V': 'Æ²Ë¬á´ á¹¼á¹¾âˆ¨â“‹ï¼¶ð•ð‘‰ð‘½ð’±ð“¥ð”™ð•ð–ð–µð—©ð˜ð™‘ðš…ðŸ„¥ðŸ……ðŸ…¥ðŸ†…ðŸ‡»',
    'W': 'Å´ÆœÉ¯É°á´¡áµ‚áº€áº‚áº„áº†áºˆâ“Œï¼·ð–ð‘Šð‘¾ð’²ð“¦ð”šð•Žð–‚ð–¶ð—ªð˜žð™’ðš†ðŸ„¦ðŸ…†ðŸ…¦ðŸ††ðŸ‡¼',
    'X': 'áºŠáºŒâ“ï¼¸ð—ð‘‹ð‘¿ð’³ð“§ð”›ð•ð–ƒð–·ð—«ð˜Ÿð™“ðš‡ðŸ„§ðŸ…‡ðŸ…§ðŸ†‡ðŸ‡½',
    'Y': 'ÃÅ¶Å¸Æ±Æ³È²ÉŽÉ¥ÊáºŽá»²á»´á»¶á»¸â“Žï¼¹ð˜ð‘Œð’€ð’´ð“¨ð”œð•ð–„ð–¸ð—¬ð˜ ð™”ðšˆðŸ„¨ðŸ…ˆðŸ…¨ðŸ†ˆðŸ‡¾',
    'Z': 'Å¹Å»Å½ÆµÈ¤Ê’Ê“á´¢áºáº’áº”â„¤â„¨â“â±«ï¼ºð™ð‘ð’ð’µð“©ð–…ð–¹ð—­ð˜¡ð™•ðš‰ðŸ„©ðŸ…‰ðŸ…©ðŸ†‰ðŸ‡¿',
    'a': 'ÂªÃ Ã¡Ã¢Ã£Ã¤Ã¥ÄÄƒÄ…ÇŽÇŸÇ¡Ç»ÈÈƒÈ§ÉÉ‘É’Î¬Î±áµƒáµ„áµ…á¸áºšáº¡áº£áº¥áº§áº©áº«áº­áº¯áº±áº³áºµáº·á¼€á¼á¼‚á¼ƒá¼„á¼…á¼†á¼‡á½°á½±á¾€á¾á¾‚á¾ƒá¾„á¾…á¾†á¾‡á¾°á¾±á¾²á¾³á¾´á¾¶á¾·â‚â’œâ“â±¥ï½ðšð‘Žð’‚ð’¶ð“ªð”žð•’ð–†ð–ºð—®ð˜¢ð™–ðšŠ',
    'b': 'Æ€ÆƒÉ“Î²Ïáµ‡áµáµ¦áµ¬á¶€á¸ƒá¸…á¸‡â’â“‘ï½‚ð›ð‘ð’ƒð’·ð“«ð”Ÿð•“ð–‡ð–»ð—¯ð˜£ð™—ðš‹',
    'c': 'Â©Ã§Ä‡Ä‰Ä‹ÄÆˆÈ¼É•Ï²á¸‰â’žâ“’ï½ƒðœð‘ð’„ð’¸ð“¬ð” ð•”ð–ˆð–¼ð—°ð˜¤ð™˜ðšŒ',
    'd': 'Ã°ÄÄ‘ÆŒÆÈ¡É–É—Î´áµˆáµŸáµ­á¶á¸‹á¸á¸á¸‘á¸“â…†â’Ÿâ““ï½„ðð‘‘ð’…ð’¹ð“­ð”¡ð••ð–‰ð–½ð—±ð˜¥ð™™ðšðŸ†¥',
    'e': 'Ã¨Ã©ÃªÃ«Ä“Ä•Ä—Ä™Ä›È…È‡È©É‡É˜É›ÉœÉÉžÎ­Îµá´ˆáµ‰áµ‹áµŒá¸•á¸—á¸™á¸›á¸áº¹áº»áº½áº¿á»á»ƒá»…á»‡á¼á¼‘á¼’á¼“á¼”á¼•á½²á½³â‚‘â„®â„¯â…‡â’ â“”ï½…ðžð‘’ð’†ð“®ð”¢ð•–ð–Šð–¾ð—²ð˜¦ð™šðšŽ',
    'f': 'Æ’áµ®á¶‚á¸Ÿâ’¡â“•ï½†ðŸð‘“ð’‡ð’»ð“¯ð”£ð•—ð–‹ð–¿ð—³ð˜§ð™›ðš',
    'g': 'ÄÄŸÄ¡Ä£Ç¥Ç§ÇµÉ É¡É¢áµáµ·á¶ƒá¸¡â„Šâ’¢â“–ï½‡ð ð‘”ð’ˆð“°ð”¤ð•˜ð–Œð—€ð—´ð˜¨ð™œðš',
    'h': 'Ä¥Ä§ÈŸÉ¦É§Ê®Ê¯Ê°Ê±á¸£á¸¥á¸§á¸©á¸«áº–â‚•â„Žâ’£â“—â±¨ï½ˆð¡ð’‰ð’½ð“±ð”¥ð•™ð–ð—ð—µð˜©ð™ðš‘',
    'i': 'Ã¬Ã­Ã®Ã¯Ä©Ä«Ä­Ä¯Ä±ÇÈ‰È‹É¨É©Î¯Î¹ÏŠá´‰áµŽáµ¢á¸­á¸¯á»‰á»‹á¼°á¼±á¼²á¼³á¼´á¼µá¼¶á¼·á½¶á½·á¾¾á¿á¿‘á¿’á¿“á¿–á¿—â±â„¹â…ˆâ’¤â“˜ï½‰ð¢ð‘–ð’Šð’¾ð“²ð”¦ð•šð–Žð—‚ð—¶ð˜ªð™žðš’ðš¤',
    'j': 'ÄµÇ°È·É‰ÉŸÊ„ÊÊ²Ï³â…‰â’¥â“™ï½Šð£ð‘—ð’‹ð’¿ð“³ð”§ð•›ð–ð—ƒð—·ð˜«ð™Ÿðš“ðš¥',
    'k': 'Ä·Ä¸Æ™Ç©ÊžÎºáµá¶„á¸±á¸³á¸µâ‚–â’¦â“šâ±ªï½‹ð¤ð‘˜ð’Œð“€ð“´ð”¨ð•œð–ð—„ð—¸ð˜¬ð™ ðš”',
    'l': 'ÄºÄ¼Ä¾Å€Å‚ÆšÆ›È´É«É¬É­Ë¡á¶…á¸·á¸¹á¸»á¸½â‚—â„“â’§â“›â±¡ï½Œð¥ð‘™ð’ð“ð“µð”©ð•ð–‘ð—…ð—¹ð˜­ð™¡ðš•',
    'm': 'É±Î¼á´Ÿáµáµšáµ¯á¶†á¸¿á¹á¹ƒâ‚˜â’¨â“œï½ð¦ð‘šð’Žð“‚ð“¶ð”ªð•žð–’ð—†ð—ºð˜®ð™¢ðš–',
    'n': 'Ã±Å„Å†ÅˆÆžÇ¹ÈµÉ²É³É´Î½áµ°á¶‡á¹…á¹‡á¹‰á¹‹â¿â‚™â’©â“ï½Žð§ð‘›ð’ð“ƒð“·ð”«ð•Ÿð–“ð—‡ð—»ð˜¯ð™£ðš—',
    'o': 'ÂºÃ²Ã³Ã´ÃµÃ¶Ã¸ÅÅÅ‘Æ¡Ç’Ç«Ç­Ç¿ÈÈÈ«È­È¯È±É”ÉµÎ¿ÏŒáµ’á¹á¹á¹‘á¹“á»á»á»‘á»“á»•á»—á»™á»›á»á»Ÿá»¡á»£á½€á½á½‚á½ƒá½„á½…á½¸á½¹â‚’â„´â’ªâ“žï½ð¨ð‘œð’ð“¸ð”¬ð• ð–”ð—ˆð—¼ð˜°ð™¤ðš˜',
    'p': 'Æ¥áµ–áµ±áµ½á¶ˆá¹•á¹—â‚šâ„—â’«â“Ÿï½ð©ð‘ð’‘ð“…ð“¹ð”­ð•¡ð–•ð—‰ð—½ð˜±ð™¥ðš™',
    'q': 'ÉŠÉ‹Ê â’¬â“ ï½‘ðªð‘žð’’ð“†ð“ºð”®ð•¢ð––ð—Šð—¾ð˜²ð™¦ðšš',
    'r': 'Â®Å•Å—Å™È‘È“ÉÉ¹ÉºÉ»É¼É½É¾É¿Ê³Ê´ÊµÊ¶áµ£áµ¨áµ²áµ³á¶‰á¹™á¹›á¹á¹Ÿâ’­â“¡ï½’ð«ð‘Ÿð’“ð“‡ð“»ð”¯ð•£ð–—ð—‹ð—¿ð˜³ð™§ðš›',
    's': 'Å›ÅÅŸÅ¡Å¿È™È¿Ê‚Ë¢Ï‚Ïƒáµ´á¶Šá¹¡á¹£á¹¥á¹§á¹©â‚›â’®â“¢ï½“ð¬ð‘ ð’”ð“ˆð“¼ð”°ð•¤ð–˜ð—Œð˜€ð˜´ð™¨ðšœ',
    't': 'Å£Å¥Å§Æ«Æ­È›È¶Ê‡ÊˆÏ„áµ—áµµá¹«á¹­á¹¯á¹±áº—â‚œâ’¯â“£â±¦ï½”ð­ð‘¡ð’•ð“‰ð“½ð”±ð•¥ð–™ð—ð˜ð˜µð™©ðš',
    'u': 'ÂµÃ¹ÃºÃ»Ã¼Å©Å«Å­Å¯Å±Å³Æ°Ç”Ç–Ç˜ÇšÇœÈ•È—É¤Ê‰Î°Ï…Ï‹Ïá´á´žáµ˜áµ™áµ¤á¹³á¹µá¹·á¹¹á¹»á»¥á»§á»©á»«á»­á»¯á»±á½á½‘á½’á½“á½”á½•á½–á½—á½ºá½»á¿ á¿¡á¿¢á¿£á¿¦á¿§â’°â“¤ï½•ð®ð‘¢ð’–ð“Šð“¾ð”²ð•¦ð–šð—Žð˜‚ð˜¶ð™ªðšž',
    'v': 'Ê‹áµ›áµ¥á¶Œá¹½á¹¿â’±â“¥ï½–ð¯ð‘£ð’—ð“‹ð“¿ð”³ð•§ð–›ð—ð˜ƒð˜·ð™«ðšŸ',
    'w': 'ÅµÊÊ·áºáºƒáº…áº‡áº‰áº˜â’²â“¦ï½—ð°ð‘¤ð’˜ð“Œð”€ð”´ð•¨ð–œð—ð˜„ð˜¸ð™¬ðš ',
    'x': 'Ë£á¶áº‹áºâ‚“â’³â“§ï½˜ð±ð‘¥ð’™ð“ð”ð”µð•©ð–ð—‘ð˜…ð˜¹ð™­ðš¡',
    'y': 'Ã½Ã¿Å·Æ´È³ÉÊŽÊ¸áºáº™á»³á»µá»·á»¹â’´â“¨ï½™ð²ð‘¦ð’šð“Žð”‚ð”¶ð•ªð–žð—’ð˜†ð˜ºð™®ðš¢',
    'z': 'ÅºÅ¼Å¾Æ¶È¥É€ÊÊ‘áµ¶á¶Žáº‘áº“áº•â’µâ“©â±¬ï½šð³ð‘§ð’›ð“ð”ƒð”·ð•«ð–Ÿð—“ð˜‡ð˜»ð™¯ðš£',
}
_ASCII_ALIKE_LOOKUP: Dict[int, str] = {ord(char): alpha for alpha, chars in _ASCII_ALIKE.items() for char in chars}


def _preprocess(text: str,
                nfkd: bool = False,
                casefold: bool = False,
                replace_ascii: bool = False,
                ) -> str:
    # sanity check
    if not isinstance(text, str):
        raise TypeError(f'expected <str>, got <{type(text)}>')

    # step 1: unicode decomposition
    if nfkd:
        text = unicodedata.normalize("NFKD", text)

    # step 2: casefold (or lowercase if we're converting to ascii later)
    if casefold:
        text = text.lower() if replace_ascii else text.casefold()

    # step 3: replace ascii-like chars
    if replace_ascii:
        text = text.translate(_ASCII_ALIKE_LOOKUP)

    return text


def word_tokenize(text: str,
                  nfkd: bool = False,
                  casefold: bool = False,
                  replace_ascii: bool = False,
                  strip_diacritics: bool = False,
                  accept_apostrophe: Union[bool, int] = False,
                  include_non_word_chars: bool = False,
                  ) -> List[str]:
    """
    tokenize text into words
    * enable `nfkd` if you want to do string matching
    * enable `casefold` if you want case-insensitivity
    * enable `replace_ascii` if you want to match ascii strings against ascii-alike text
    * enable `strip_diacritics` if you want to fix zalgo text or want to ignore accents
    * enable `accept_apostrophe` if you want to allow an apostrophe in your word (not recommended)

    warning: if any flags are enabled, the output words may not be substrings of the input text

    :param text: to extract words from
    :param nfkd: unicode normal form compatibility decomposition
    :param casefold: lowercase but better
    :param replace_ascii: make ascii-like where possible
    :param strip_diacritics: unwrap graphemes, keeping only initial codepoint
    :param accept_apostrophe: allow an apostrophe, or an integer number of apostrophes
    :param include_non_word_chars: include non-word (e.g. whitespace) chars in the output token list
    :return: list of words
    """
    # sanity check
    assert isinstance(accept_apostrophe, (bool, int)) and accept_apostrophe >= 0

    # pre-process (and sanity check)
    text = _preprocess(text, nfkd=nfkd, casefold=casefold, replace_ascii=replace_ascii)

    # get all word tokens
    words = []
    word_buffer = []
    apostrophe_locations = []
    for match in _REGEX_GRAPHEME.finditer(f'\2{text}\3'):  # ensure first and last iterations are not graphemes
        grapheme = match.group(0)
        char = grapheme[0]

        # get all word-like graphemes
        # in my opinion, underscore is not word-like (despite what the unicode standard says)
        if _REGEX_WORD_CHAR.match(char):
            if char not in {'_'}:
                word_buffer.append(char if strip_diacritics else grapheme)
                continue

        # allow adding any number of apostrophes to words
        apostrophe_chars = set("'\u2019\uFF07")
        # we'll filter out words with too many apostrophes later
        if char in apostrophe_chars:
            apostrophe_locations.append(len(word_buffer))
            word_buffer.append(char if strip_diacritics else grapheme)
            continue

        # not a word-like grapheme, so clear word buffer
        if word_buffer:

            # if we have an acceptable number (possibly zero) of apostrophes
            if len(apostrophe_locations) <= accept_apostrophe:
                words.append(''.join(word_buffer))

            # otherwise split at the known apostrophe locations
            else:
                current_idx = 0
                for next_idx in apostrophe_locations:
                    if next_idx > current_idx:
                        words.append(''.join(word_buffer[current_idx:next_idx]))
                    current_idx = next_idx + 1
                if current_idx < len(word_buffer):  # don't forget to add the last bit, if any
                    words.append(''.join(word_buffer[current_idx:len(word_buffer)]))

            # clear buffers and keep looking for more words
            word_buffer.clear()
            apostrophe_locations.clear()

        if include_non_word_chars:
            words.append(char)

    # don't return the \2 and \3
    return words[1:-1] if include_non_word_chars else words


if __name__ == '__main__':
    print(json.dumps(word_tokenize('hello')))
    print(json.dumps(word_tokenize('hello world')))
    print(json.dumps(word_tokenize('ð‡ðžð¥ð¥ð¨ ð–ð¨ð«ð¥ð', nfkd=True)))
    print(json.dumps(word_tokenize('ð—›ð—²ð—¹ð—¹ð—¼ ð—ªð—¼ð—¿ð—¹ð—±', nfkd=True)))
    print(json.dumps(word_tokenize('ð»ð‘’ð‘™ð‘™ð‘œ ð‘Šð‘œð‘Ÿð‘™ð‘‘', nfkd=True)))
    print(json.dumps(word_tokenize('ð˜ð˜¦ð˜­ð˜­ð˜° ð˜žð˜°ð˜³ð˜­ð˜¥', nfkd=True)))
    print(json.dumps(word_tokenize('ð‘¯ð’†ð’ð’ð’ ð‘¾ð’ð’“ð’ð’…', nfkd=True)))
    print(json.dumps(word_tokenize('ð™ƒð™šð™¡ð™¡ð™¤ ð™’ð™¤ð™§ð™¡ð™™', nfkd=True)))
    print(json.dumps(word_tokenize('ï¼µï½Žï½‰ï½ƒï½ï½„ï½…!', replace_ascii=True)))
    print(json.dumps(word_tokenize('ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½', replace_ascii=True)))
    print(json.dumps(word_tokenize('ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª!', replace_ascii=True, strip_diacritics=True)))
    print(json.dumps(word_tokenize("   'a   a'a   a'   "
                                   "   ''a   'a'a   'a'   a'a'a   a'a'   a''   "
                                   "   '''a ''a'a 'a''a 'a'a'a a'a'a'a 'a'a' a'a'a' a''a' a'a'' a'''   ",
                                   accept_apostrophe=2)))
